---
title: "Take a close look at Jazz piano music and Classical piano music"
author: "Clover"
date: "20 February 2019"
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: lumen
---

```{r}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(protoclust)
library(heatmaply)
library(spotifyr)
library(compmus)
source('spotify.R')
```

### Introduction and Corpus

**Introduction**

Jazz and Classical music are different. The main difference is probably because when you playing jazz, you have to improvise and compose on the spot and range things, however, with classical music, you are playing all the notes composed. That is, Jazz performers play freestyle music, while classical music performers play pre-structured music.

A study even proved that there is a different process occurred in the brains of the jazz and classical pianists. In particular, the jazz pianists' brains began re-planning sooner than the classical pianists' brains. Classical pianists tend to focus on 'how' to play the music. This means their focus is on technique and the personal expression they add to the piece. Jazz pianists, on the other hand, focus on  ‘what’ to play, meaning they are always prepared to improvise and adapt the notes they're playing. 

2 music genre under 2 contrary mindsets are different. Thus,this storyboard is going to analyzing the difference between jazz and classical music trying to illustrate what is the major difference between such freestyle and disciplined music by graphs and to capiture how the mind set per genre influences the sound.

The analysis will first show the differences comparing 2 typical and popular songs from 2 music genre and then take a general look at the 2 genre samples.

**Corpus**

To completed the research focusing on the *piano* songs of both music genres ( acousticness>0.9 for all the tracks), 2 Spotify playlists will be taken. The most popular song from each album as representatives to do song specific analysis. 2 songs are: "Peace piece" by Bill Evans from the Jazz album, and "First love" by Yiruma from the Classical album.

| Playlist | Name | Discription | Number of tracks | Popular song |
| :-: | :-: | - |:-: |:-:
| ![Jazz](playlist11.png)| [Jazz Piano seleted by Jazz Pianoists](https://open.spotify.com/user/jasonluxmusic/playlist/621IKNkRFQQkauQygKcrPC?si=IrbnsHjhRuiDFekHGD9pbg)  | Jazz piano pieces selected by pianists. Solo acoustic piano only; no rhythm or vocals.| 65 | Peace piece _ Bill Evans |
| ![Classical](playlist22.png)| [Classical Piano Essentials](https://open.spotify.com/user/doublejmusic/playlist/5ifMsPihlkYEGCakqfqj17?si=kPoYH712REKzdDsE6Dxf_A) | The world's greatest piano songs, featuring Fur Elise, Clair De Lune, River flows in you etc.) | 135 | First love _ Yiruma |

Spotify API have 12 features for a track, namely, key, mode, time_signature, acousticness, danceability, energy, instrumentalness, liveness, loudness, speechiness, valence, tempo. To see explanation for each variable, please click "[Here](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/)"

### Chromagrams features of 2 songs

```{r}
First_love <- 
  get_tidy_audio_analysis('47ZWr1Nb0PUgmKgyg5JkgO') %>% 
  select(segments) %>% unnest(segments) %>% 
  select(start, duration, pitches)

First_love %>% 
  mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
  compmus_gather_chroma %>% 
  ggplot(
    aes(
      x = start + duration / 2, 
      width = duration, 
      y = pitch_class, 
      fill = value)) + 
  geom_tile() +
  labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
  theme_minimal() +
  ggtitle("River Flows in You by Yiruma")

Peace_Piece <- 
  get_tidy_audio_analysis('58yFroDNbzHpYzvicaC0de') %>% 
  select(segments) %>% unnest(segments) %>% 
  select(start, duration, pitches)

Peace_Piece %>% 
  mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
  compmus_gather_chroma %>% 
  ggplot(
    aes(
      x = start + duration / 2, 
      width = duration, 
      y = pitch_class, 
      fill = value)) + 
  geom_tile() +
  labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
  theme_minimal() +
  ggtitle("Peace Piece by Bill Evans") 

```

***

Chromagrams at the left show the intensity of the twelve pitch classes for each song per small fragment while the music is playing. One interesting fact is that “Peace piece” from the Jazz list prefers playing G a lot in the key of C major with relatively gentle harmony showed by the lighter blue in other pitch classes, while Yiruma seems to stick to the three major keys when playing a song in A major.

In addition, in "Peace piece",  the song is with a strong preference on G with some mention of B, A,  E and C. However, the melody of  "river flows in you" seems more “colorful” with B, A, F#, E, D and  C#. No basis for any keys.


### Chordgram - Peace piece by Bill Evans

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3),
)

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))


Peace_Key <- 
    get_tidy_audio_analysis('58yFroDNbzHpYzvicaC0de') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    compmus_match_pitch_template(chord_templates, 'angular', 'euclidean') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', fill = 'Distance') +
    ggtitle("Peace Piece by Bill Evans") 
ggplotly(Peace_Key)
```

***

The chordgram at the left shows the chords in "Peace piece". The piece is segmented according to Spotify’s estimates, and the distances represented are 	cosine distances from Spotify’s chroma vectors of the keys in the **y** axis. The song is in C chords with A7 chords.


### Chordgram - River flows in you by Yiruma

```{r}
First_Key <- 
    get_tidy_audio_analysis('47ZWr1Nb0PUgmKgyg5JkgO') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    compmus_match_pitch_template(chord_templates, 'angular', 'euclidean') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'E') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', fill = 'Distance') +
    ggtitle("River Flows in You by Yiruma")
ggplotly(First_Key)
```

***

The chordgram at the left shows the chords dist in "River flows in you". The piece is segmented according to Spotify’s estimates, and the distances represented are 	cosine distances from Spotify’s chroma vectors of the keys in the **y** axis. The whole song is on A chords.


### Self-Similarity Matrices - Peace piece by Bill Evans

```{r}
Peace_self <- 
    get_tidy_audio_analysis('58yFroDNbzHpYzvicaC0de') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))
Peace_pic <- 
    bind_rows(
        Peace_self %>% compmus_self_similarity(pitches, 'aitchison') %>% mutate(d = d / max(d), type = "Chroma"),
        Peace_self %>% compmus_self_similarity(timbre, 'euclidean') %>% mutate(d = d / max(d), type = "Timbre")) %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    facet_wrap(~ type) +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '') +
    ggtitle("Peace Piece by Bill Evans") 
ggplotly(Peace_pic)


```

***

Self-similarity matrics graphically depict the similarity of song itself.
One audio file is represented as a square. Each side of the square is proportional to the length of the piece, and time runs from left to right as well as from bottom to top. Similar regions are dark  while dissimilar regions are  the novelty which is illustrated by the yellow line. Thus there is always a dark diagonal line running from bottom left to top right because the audio is always the most similar to itself at any particular time.
The Chroma and timbre grams do not show a huge difference is because both tracks are played only by piano. However, the music structure between 2 songs differs. 

For the track "peace by piece", the song suddenly changes after 2 mins and have another new section after 5 mins.It does not have a obvious repeated part after the intro.

### Self-Similarity Matrices - River flows in you by Yiruma

```{r}
First_self <- 
    get_tidy_audio_analysis('47ZWr1Nb0PUgmKgyg5JkgO') %>% 
    compmus_align(bars, segments) %>% 
    select(bars) %>% unnest(bars) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'acentre', norm = 'manhattan')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))
First_pic <- 
    bind_rows(
        First_self %>% compmus_self_similarity(pitches, 'aitchison') %>% mutate(d = d / max(d), type = "Chroma"),
        First_self %>% compmus_self_similarity(timbre, 'euclidean') %>% mutate(d = d / max(d), type = "Timbre")) %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    facet_wrap(~ type) +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '')  +
  ggtitle("River Flows in You by Yiruma")
ggplotly(First_pic)


```

***

By contrast, "River flows in you" seems to be more structured, reflecting by the regularsequential squares and yellow lines.


### Classification Algorithms

```{r}
Jazz_120<- get_playlist_audio_features('jasonluxmusic', '621IKNkRFQQkauQygKcrPC') %>% 
    slice(1:67) %>% 
    add_audio_analysis
Classical_120 <- get_playlist_audio_features('doublejmusic', '5ifMsPihlkYEGCakqfqj17') %>% 
    slice(1:140) %>% 
    add_audio_analysis

indie <- 
    Jazz_120 %>% mutate(playlist = "Jazz") %>% 
    bind_rows(
        Classical_120 %>% mutate(playlist = "Classical")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)

indie_class <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B,
           data = indie) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(indie) %>% 
    juice

indie_cv <- indie_class %>% vfold_cv(10)

indie_knn <- nearest_neighbor(neighbors = 1) %>% set_engine('kknn')
predict_knn <- function(split)
    fit(indie_knn, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))

indie_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class)

indie_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap')
```

***

After 2 songs anaylsis, let's, then, take a look from a general view.

The heatmap at the left shows the performance of a classifier trying to distinguish the jazz playlist from the classical playlist. It works with the whole sample set with 140 Classical songs and 67 Jazz songs.

It displays the results, after 10-fold cross-validation, of a nearest-neighbour classifier with 10 Spotify features (danceability, energy, loudness, speechiness, acousticness , instrumentalness, liveness, valence, tempo,  durations) and 12 pitch classes of the song. 

24 over 116 (\approx 20%) classical music is classified in the wrong class whereas 19 over 67 (\approx 28%) Jazz music is classified wrongly. The performance of the classifier is above average: overall classification accuracy is 78%, with a J-index of 0.53. The precision of the classifier’s predictions of Jazz music is lower than the one predicts Classical. 


### 4 major track differences between 2 list {data-commentary-width=500}

```{r}
#Jazz
Jazz <- get_playlist_audio_features('jasonluxmusic', '621IKNkRFQQkauQygKcrPC') %>%
  arrange(track_popularity) %>%
  mutate(track_name = paste(seq(length(track_name)), track_name)) 

#Classical
Classical <- get_playlist_audio_features('doublejmusic', '5ifMsPihlkYEGCakqfqj17') %>%
  arrange(track_popularity) %>%
  mutate(track_name = paste(seq(length(track_name)), track_name)) 

#Data
Dataset <-
  Jazz %>% mutate(genre = "Jazz") %>%
  bind_rows(Classical %>% mutate(genre = "Classical")) 

#Image
  LMVE <- Dataset %>%                       # Start with awards.
  ggplot(                         # Set up the plot.
    aes(
      x = loudness,
      y = valence,
      size = energy,
      colour = mode
    )
  ) +
  geom_point(alpha = 0.8) +               # Scatter plot.
  geom_rug(size = 0.001) +       # Add 'fringes' to show data distribution.
  facet_wrap(~ genre) +     # Separate charts per playlist.
  scale_x_continuous(          # Fine-tune the x axis.
    limits = c(-45, -8),
    breaks = c(-45, 5, -8),  # Use grid-lines for quadrants only.
    minor_breaks = NULL      # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 0.9),
    breaks = c(0, 0.2, 0.9),
    minor_breaks = NULL
  ) +
  scale_color_manual(values=c("#E69F00", "#56B4E9")) +
  scale_size_continuous(       # Fine-tune the sizes of each point.
    trans = "exp",           # Use an exp transformation to emphasise loud.
    guide = "none"           # Remove the legend for size.
  ) +
  theme_light() +              # Use a simpler them.
  labs(                        # Make the titles nice.
    x = "loudness",
    y = "valence",
    size = "energy",
    colour = "Mode"
  )
ggplotly(LMVE)

```

***

Using a random-forest classifier, the most important features for classifying tracks among these playlists are:**loudness, energy, valance**.

This graph at the left, therefore, is made to shows the above 3 dimensions plus modes of Jazz and Classical music. The *x* axis shows 'loudness' and the *y* axis shows Spotify’s 'valence’ feature.  The major mode is in <font color=#E69F00>orange</font>  whereas the minor mode is in <font color=#56B4E9>blue</font>. The size of each dot represents the energy of each track.

In Jazz tracks, the proportion of **majors** ( *88%* ) is relatively larger than the proportion of major in Classical music list (*63%*).  Although Jazz modes are more or less different from the tradition scale,  it seems like Spotify prefers to classifying them into major. 

The **loudness** is more varied for Classical music. It is mainly distributed from -26 to -13 in Jazz list, but this range is larger in Classical music list where it is distributed from -36 to -15 with a higher standard deviation judging by the more scattered dots. Comparing with the relatively similar **valence** feature in the Classical list (except the highest and lowest valence tracks), Classical music is peaceful on average whereas Jazz music is evenly distributed across different energy. According to the dots size and the figures in Figure 2, Classical music is peaceful on average whereas Jazz music is evenly distributed across different energy. 

All in all, the visualisation shows that Jazz music is (according to Spotify) rather louder and rather energetic than the Classical music. This may tell that as a freestyle music genre, Jazz players put more emphasizing on the role of **melogy, rhythm and emotion**.

**Figure 1.** A summary of the mean of each variable.

| Genre | M_danancibility | ***M_enerergy*** | ***M_loudness*** | M_speechiness | M_liveness | ***M_valence*** | M_tempo | M_instrumentalness | M_acousticness |
| :-: |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Jazz| 0.423 | *0.127* | *-20.6* | 0.0544  | 0.162 | *0.179* | 96.9  | 0.887 |  0.981  | 
| Classialc| 0.376 | *0.0609*  | *-27.0* | 0.0562 |  0.103 | *0.226* |  93.0 | 0.895 | 0.991 |


**Figure 2.** Energy distrybution (Vertical dotted line represents the average value of each genre.)

![energy](energy.png)



### Hierarchical Clustering

```{r}
Jazz_clus <- 
    Jazz[1:50,] %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)
Jazz_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = Jazz_clus) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    step_range(all_predictors()) %>% 
    prep(Jazz_clus %>% mutate(track_name = str_trunc(track_name, 10))) %>% 
    juice %>%
    column_to_rownames('track_name')
Jazz_dist <- dist(Jazz_juice, method = 'euclidean')
hclust(Jazz_dist, method = 'average') %>% dendro_data %>% ggdendrogram + ggtitle('Jazz')

Classic_clus <- 
    Classical[1:50,] %>% 
    add_audio_analysis %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(pitches, timbre)
Classic_juice <- 
    recipe(track_name ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration_ms +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = Classic_clus) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    step_range(all_predictors()) %>% 
    prep(Classic_clus %>% mutate(track_name = str_trunc(track_name, 10))) %>% 
    juice %>%
    column_to_rownames('track_name')
Classic_dist <- dist(Classic_juice, method = 'euclidean')
hclust(Classic_dist, method = 'average') %>% dendro_data %>% ggdendrogram + ggtitle('Classical')
```

***

The hierarchical clustering diagram on the left shows the similarities of the songs in each playlist. For convenient reading and comparing, I take the top 50 songs from each playlist to make the comparison, where the first graph shows the Jazz playlist and the second graph shows the Classical playlist.

By hearing the 4 songs listed on the left and right side of the clustering gram, there is a clear difference of tempo and intensity.

Overall,  there are more subsets in classical music and the clustering trees are higher. This explains that Jazz music is relatively more similar to each other. This probably implies that although Jazz is freestyle music, Jazz musicians are not completely ”free“. There might be some structure in their mind to playing the melody to make sure the next pop up key will not sound "wrong". The structure may be highly correlated to tempo or mode. However, Classical music, as pre-written music, can be revised over and over again before it officially released. It has the opportunity to try and correct until the composer is satisfied with his master piece.

### Conclusion

Jazz and Classical music are 2 music genres with regard to the different music theory background. Furthermore, Jazz music usually is a freestyle performance different from Classical music. Therefore, This storyboard aims to show whether 2 contrary style performance  will lead to distincble features between Jazz and Classical piano music based on 2 specific tracks level and the 2 playlists.

By look at 2 representative songs,  Jazz track shows a high novelty in the music structure and not monotonous melody, compared with the classical track. It does show a high - freestyle level.

In general,  Jass and Classical music can be distinguished with 3 main different features on loudness, energy, valance, where Jazz is louder and energetic on average. But as a music genre famous of improvising, each Jazz music track seems not to vary from each other as much as the classical music. Jazz musicians may not be as free as we thought. 

Further research, therefore, can be conducted to figure out what is highly correlated in Jazz music and what is not. The consistency might be because of the tempo or the same chroma in different modalities.



Clover
03.2019